{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a69d8-7441-480a-9d65-d9f1fdce5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re, glob, sys\n",
    "from llc_uv_shift import llc_uv_shift\n",
    "import temp_anom_budgets as tab\n",
    "from dask import delayed, compute\n",
    "from cal_wmb_llc import vertical_pairwise_avg\n",
    "import time\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import xgcm\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# === 基本配置 ===\n",
    "data_dir = '/dfs9/hfdrake_hpc/datasets/ECCOv4r4/'\n",
    "grid_path = 'GRID_GEOMETRY_ECCO_V4r4_native_llc0090.nc'\n",
    "mask_path = 'goa_mask_llc90.nc'\n",
    "rho_pot = 1029.0\n",
    "rhoconst = 1029.0\n",
    "face_connections = {'face':\n",
    "                    {0: {'X':  ((12, 'Y', False), (3, 'X', False)),\n",
    "                         'Y':  (None,             (1, 'Y', False))},\n",
    "                     1: {'X':  ((11, 'Y', False), (4, 'X', False)),\n",
    "                         'Y':  ((0, 'Y', False),  (2, 'Y', False))},\n",
    "                     2: {'X':  ((10, 'Y', False), (5, 'X', False)),\n",
    "                         'Y':  ((1, 'Y', False),  (6, 'X', False))},\n",
    "                     3: {'X':  ((0, 'X', False),  (9, 'Y', False)),\n",
    "                         'Y':  (None,             (4, 'Y', False))},\n",
    "                     4: {'X':  ((1, 'X', False),  (8, 'Y', False)),\n",
    "                         'Y':  ((3, 'Y', False),  (5, 'Y', False))},\n",
    "                     5: {'X':  ((2, 'X', False),  (7, 'Y', False)),\n",
    "                         'Y':  ((4, 'Y', False),  (6, 'Y', False))},\n",
    "                     6: {'X':  ((2, 'Y', False),  (7, 'X', False)),\n",
    "                         'Y':  ((5, 'Y', False),  (10, 'X', False))},\n",
    "                     7: {'X':  ((6, 'X', False),  (8, 'X', False)),\n",
    "                         'Y':  ((5, 'X', False),  (10, 'Y', False))},\n",
    "                     8: {'X':  ((7, 'X', False),  (9, 'X', False)),\n",
    "                         'Y':  ((4, 'X', False),  (11, 'Y', False))},\n",
    "                     9: {'X':  ((8, 'X', False),  None),\n",
    "                         'Y':  ((3, 'X', False),  (12, 'Y', False))},\n",
    "                     10: {'X': ((6, 'Y', False),  (11, 'X', False)),\n",
    "                          'Y': ((7, 'Y', False),  (2, 'X', False))},\n",
    "                     11: {'X': ((10, 'X', False), (12, 'X', False)),\n",
    "                          'Y': ((8, 'Y', False),  (1, 'X', False))},\n",
    "                     12: {'X': ((11, 'X', False), None),\n",
    "                          'Y': ((9, 'Y', False),  (0, 'X', False))}}}\n",
    "\n",
    "prefix = {\n",
    "    'ts':      'OCEAN_TEMPERATURE_SALINITY_snap/OCEAN_TEMPERATURE_SALINITY_snap',\n",
    "    'tsdaily': 'ECCO_L4_TEMP_SALINITY_LLC0090GRID_DAILY_V4R4/OCEAN_TEMPERATURE_SALINITY_day_mean_',\n",
    "    'tadv':    'ECCO_L4_OCEAN_3D_TEMPERATURE_FLUX_LLC0090GRID_DAILY_V4R4/OCEAN_3D_TEMPERATURE_FLUX_day_mean_',\n",
    "    'hflux':   'ECCO_L4_HEAT_FLUX_LLC0090GRID_DAILY_V4R4/OCEAN_AND_ICE_SURFACE_HEAT_FLUX_day_mean_',\n",
    "    'ssh':     'SEA_SURFACE_HEIGHT_snap/SEA_SURFACE_HEIGHT_snap_',\n",
    "    'sshdaily': 'ECCO_L4_SSH_LLC0090GRID_DAILY_V4R4/SEA_SURFACE_HEIGHT_day_mean_',\n",
    "    'sflux':   'ECCO_L4_FRESH_FLUX_LLC0090GRID_DAILY_V4R4/OCEAN_AND_ICE_SURFACE_FW_FLUX_day_mean_',\n",
    "    'volflux': 'ECCO_L4_OCEAN_3D_VOLUME_FLUX_LLC0090GRID_DAILY_V4R4/OCEAN_3D_VOLUME_FLUX_day_mean_',\n",
    "    'vstar':   'ECCO_L4_BOLUS_LLC0090GRID_DAILY_V4R4/OCEAN_BOLUS_VELOCITY_day_mean_'\n",
    "}\n",
    "\n",
    "clim_dir = data_dir\n",
    "#clim_files = {\n",
    "#    \"tadv\":            \"tadv_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#    \"hflux\":          \"hflux_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"sflux\":         \"sflux_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"tsdaily\":     \"tsdaily_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"vstar\":         \"vstar_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"vol\":             \"vol_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"tsdailyS\":   \"tsdailyS_clim_1992-2017_30dMA_allDays.nc\",\n",
    "#     \"eddyforcing\": \"dailyclim_eddyforcing.nc\"\n",
    "# }\n",
    "clim_files = {\n",
    "    \"tadv\":            \"tadv_clim_1992-2017_raw_allDays.nc\",\n",
    "    \"hflux\":          \"hflux_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"sflux\":         \"sflux_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"tsdaily\":     \"tsdaily_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"vstar\":         \"vstar_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"vol\":             \"vol_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"tsdailyS\":   \"tsdailyS_clim_1992-2017_raw_allDays.nc\",\n",
    "     \"eddyforcing\": \"dailyclim_eddyforcing_raw.nc\"\n",
    " }\n",
    "\n",
    "clim_keep_vars = {\n",
    "     \"tadv\":        [\"DFxE_TH\", \"DFyE_TH\", \"DFrE_TH\", \"DFrI_TH\"],\n",
    "     \"tsdailyS\":    [\"THETA\"],\n",
    "     \"eddyforcing\": [\"Hnabla_eddy\", \"Vnabla_eddy\"],\n",
    "     \"tsdaily\":     [\"THETA\"]\n",
    " }\n",
    "\n",
    "def sel_and_retime(path: str, mmdd_seq: np.ndarray, time_coord: xr.DataArray, keep=None) -> xr.Dataset:\n",
    "    ds = xr.open_dataset(path).chunk(chunks={\"mmdd\": 1})\n",
    "    if keep:\n",
    "        ds = ds[keep]\n",
    "    ds = ds.sel(mmdd=mmdd_seq).drop_vars('mmdd').rename({'mmdd': 'time'})\n",
    "    return ds.assign_coords(time=time_coord).reset_coords(drop=True)\n",
    "def sel_and_retime_clim(key: str, mmdd_seq, time_coord):\n",
    "    path = os.path.join(clim_dir, clim_files[key])\n",
    "    keep = clim_keep_vars.get(key, None)\n",
    "    return sel_and_retime(path, mmdd_seq, time_coord, keep=keep)\n",
    "def align_time_like(da_target: xr.Dataset, da_ref: xr.Dataset) -> xr.Dataset:\n",
    "    ref_dates = da_ref.time.dt.floor('D').values\n",
    "    ref_times = da_ref.time.values\n",
    "    mapping = dict(zip(ref_dates, ref_times))\n",
    "    tgt_dates = da_target.time.dt.floor('D').values\n",
    "    new_times = np.array([mapping.get(d, t) for d, t in zip(tgt_dates, da_target.time.values)])\n",
    "    da_target = da_target.copy()\n",
    "    da_target['time'] = new_times\n",
    "    return da_target\n",
    "# 查找下月1号的文件\n",
    "def find_next_day_file(prefix_path, next_day):\n",
    "    date_str = next_day.strftime('%Y-%m-%d')\n",
    "    pattern = os.path.join(data_dir, f\"{prefix_path}*{date_str}T000000*.nc\")\n",
    "    files = glob.glob(pattern)\n",
    "    return files[0] if files else None\n",
    "    \n",
    "def get_month_str(filename):\n",
    "    patterns = [\n",
    "        r'_(\\d{4})-(\\d{2})-\\d{2}T\\d{6}_',  # 有T的格式\n",
    "        r'_(\\d{4})-(\\d{2})-\\d{2}_'         # 无T的格式\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return f\"{match.group(1)}{match.group(2)}\"\n",
    "    raise ValueError(f\"No valid date found in filename: {filename}\")\n",
    "\n",
    "\n",
    "# === 时间设置 ===\n",
    "#\n",
    "month = '199202'\n",
    "# 从命令行获取年月，格式如199201\n",
    "#if len(sys.argv) < 2:\n",
    "#    raise ValueError(\"请提供 YYYYMM 参数\")\n",
    "#month = sys.argv[1]\n",
    "#outputdir = f'G_global_wmb_anomaly_{month}.nc'\n",
    "outputdir = f'G_goa_wmb_anomaly_{month}.nc'\n",
    "\n",
    "# 如果输出文件已存在，则跳过计算\n",
    "if os.path.exists(outputdir):\n",
    "    print(f\"✅ 输出文件已存在：{outputdir}，跳过计算\")\n",
    "    os.remove(outputdir)\n",
    "    #sys.exit(0)  # 或者 return / exit，根据你是否在函数内\n",
    "\n",
    "this_month = pd.to_datetime(month, format='%Y%m')\n",
    "next_month_first_day = this_month + pd.DateOffset(months=1)\n",
    "\n",
    "# 主文件收集逻辑\n",
    "day_files = {}\n",
    "has_next_month_data = {}\n",
    "\n",
    "for key, pref in prefix.items():\n",
    "    # 当前月的所有文件\n",
    "    all_files = sorted(glob.glob(os.path.join(data_dir, pref + '*.nc')))\n",
    "    current_month_files = [f for f in all_files if get_month_str(f) == month]\n",
    "\n",
    "    # 仅对 ts 和 ssh 添加下月第一天\n",
    "    if key in ['ts', 'ssh']:\n",
    "        next_day_file = find_next_day_file(pref, next_month_first_day)\n",
    "        if next_day_file:\n",
    "            current_month_files.append(next_day_file)\n",
    "            has_next_month_data[key] = True\n",
    "        else:\n",
    "            print(f\"{month} ➤ {key}: ❌ 没有下月1号的数据\")\n",
    "            has_next_month_data[key] = False\n",
    "    day_files[key] = current_month_files\n",
    "\n",
    "\n",
    "# === 网格与掩码 ===\n",
    "ECCOgrid = xr.open_dataset(grid_path).rename({'tile': 'face'}).load()\n",
    "mask = xr.open_dataarray(mask_path).load()\n",
    "rA, drF, hFacC, Depth = ECCOgrid['rA'], ECCOgrid['drF'], ECCOgrid['hFacC'], ECCOgrid['Depth']\n",
    "vol = (rA * drF * hFacC).transpose('face', 'k', 'j', 'i').astype('float64')\n",
    "\n",
    "# === 懒加载数据 ===\n",
    "print(f\"Processing {month} ...\")\n",
    "\n",
    "\n",
    "open3d = dict(chunks={'time':1, 'k':50, 'k_l':50, 'face':13, 'j':90, 'i':90, 'i_g':90, 'j_g':90, 'tile':13, 'face': 13})\n",
    "open2d = dict(chunks={'time':1, 'face':13, 'j':90, 'i':90, 'i_g':90, 'j_g':90, 'tile':13, 'face': 13})\n",
    "\n",
    "#\n",
    "testdays = 2\n",
    "#\n",
    "\n",
    "def preprocess(ds):\n",
    "    return ds.rename({'tile':'face'})\n",
    "\n",
    "tscache  = xr.open_mfdataset(day_files['ts'][0:testdays+1], preprocess=preprocess, chunks=open3d[\"chunks\"])\n",
    "tscache  = tscache.assign_coords( time=tscache.time.dt.floor(\"D\") + pd.Timedelta(hours=12) )\n",
    "print(\"load ts done!\")\n",
    "tsdaily  = xr.open_mfdataset(day_files['tsdaily'][0:testdays], preprocess=preprocess, chunks=open3d[\"chunks\"])\n",
    "#tsdaily = tsdaily.reset_coords(drop=True)\n",
    "print(\"load tsdaily done!\")\n",
    "tbudget  = xr.open_mfdataset(day_files['tadv'][0:testdays], preprocess=preprocess, chunks=open3d[\"chunks\"])\n",
    "print(\"load tadv done!\")\n",
    "hflux    = xr.open_mfdataset(day_files['hflux'][0:testdays], preprocess=preprocess, chunks=open2d[\"chunks\"])\n",
    "print(\"load hflux done!\")\n",
    "ssh      = xr.open_mfdataset(day_files['ssh'][0:testdays+1], preprocess=preprocess, chunks=open2d[\"chunks\"])\n",
    "ssh      = ssh.assign_coords( time=ssh.time.dt.floor(\"D\") + pd.Timedelta(hours=12) )\n",
    "print(\"load ssh done!\")\n",
    "sshdaily = xr.open_mfdataset(day_files['sshdaily'][0:testdays], preprocess=preprocess, chunks=open2d[\"chunks\"])\n",
    "print(\"load sshdaily done!\")\n",
    "sflux    = xr.open_mfdataset(day_files['sflux'][0:testdays], preprocess=preprocess, chunks=open2d[\"chunks\"])\n",
    "print(\"load sflux done!\")\n",
    "volflux  = xr.open_mfdataset(day_files['volflux'][0:testdays], preprocess=preprocess, chunks=open3d[\"chunks\"])\n",
    "print(\"load volflux done!\")\n",
    "vstar    = xr.open_mfdataset(day_files['vstar'][0:testdays], preprocess=preprocess, chunks=open3d[\"chunks\"])\n",
    "print(\"load vstar done!\")\n",
    "\n",
    "#\n",
    "grid_snap = xgcm.Grid(tscache, periodic=False, face_connections=face_connections )\n",
    "grid_daily = xgcm.Grid(tsdaily, periodic=False, face_connections=face_connections )\n",
    "\n",
    "#\n",
    "\n",
    "# === 时间对齐 ===\n",
    "#ssh = align_time_like(ssh, tbudget)\n",
    "#tscache = align_time_like(tscache, tbudget)\n",
    "# === 气候态数据 ===\n",
    "mmdd_seq = tbudget.time.dt.strftime('%m%d').values\n",
    "clim = {k: sel_and_retime_clim(k, mmdd_seq, tbudget.time) for k in clim_files}\n",
    "mmdd_seq = tscache.time.dt.strftime('%m%d').values\n",
    "clim_tsdaily = sel_and_retime_clim('tsdaily', mmdd_seq, tscache['THETA'].time)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n",
    "\n",
    "print(\"loading complete\")\n",
    "print(\"preparing Tprime budget ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39867f8d-f9c7-4d94-ab9f-17ccdc853078",
   "metadata": {},
   "source": [
    "## using client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96508c5-de61-42fb-aabc-dd49831cc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask, os, sys\n",
    "\n",
    "#\n",
    "my_dashboard_port = 8809#8501 # pick a unique one!\n",
    "#\n",
    "\n",
    "os.environ[\"DASK_DISTRIBUTED__DASHBOARD__ADDRESS\"] = f\"0.0.0.0:{my_dashboard_port}\"\n",
    "# extra resilience: retry tasks when workers die\n",
    "dask.config.set({\n",
    "    \"distributed.scheduler.default-task-retries\": 3,   # try failed tasks 3x\n",
    "    \"distributed.worker.memory.target\": 0.85,          # start spilling earlier\n",
    "    \"distributed.worker.memory.spill\": 0.90,\n",
    "    \"distributed.worker.memory.terminate\": 0.98,\n",
    "})\n",
    "\n",
    "# tiny scheduler + notebook are already on your standard node\n",
    "# now create *workers* on the free partition\n",
    "cluster = SLURMCluster(\n",
    "    queue=\"free\",                 # <- partition for workers\n",
    "    walltime=\"03:00:00\",          # preemptible jobs usually shorter; autoscale will re-submit\n",
    "    cores=2,                      # per worker *job* (processes*threads below should ~= cores)\n",
    "    processes=1,                  # n workers per job\n",
    "    memory=\"32GB\",                # total per job\n",
    "    local_directory=\"/tmp\",       # fast node-local spill\n",
    "    job_extra_directives=[\n",
    "        \"--qos=free-part\",\n",
    "        \"--requeue\",\n",
    "        \"--signal=TERM@120\",\n",
    "    ],\n",
    "    python=sys.executable,\n",
    "    job_script_prologue=[\n",
    "        \"set -euo pipefail\",\n",
    "        \"export HDF5_USE_FILE_LOCKING=FALSE\",\n",
    "        \"export DASK_TEMPORARY_DIRECTORY=/tmp\",\n",
    "    ],\n",
    "    scheduler_options={\"dashboard_address\": f\"0.0.0.0:{my_dashboard_port}\"},\n",
    ")\n",
    "\n",
    "client = Client(cluster)\n",
    "\n",
    "# Elastic scale: grow to use free cycles; shrink when idle\n",
    "cluster.adapt(minimum_jobs=1, maximum_jobs=24, wait_count=3, interval=\"10s\")\n",
    "\n",
    "print(\"Scheduler:\", client)\n",
    "print(\"Dashboard:\", client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6221d6-ccc7-4c54-8cad-422d5922f2f7",
   "metadata": {},
   "source": [
    "## calculation of advection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6fbd6-f916-449e-a75b-d7b690a0ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 7) 计算各项 —— 全部保持 lazy 直到写文件\n",
    "# 7.1 体积（含 ETAN 拉伸）\n",
    "s_star = 1.0 + ssh.ETAN / Depth  # 无量纲\n",
    "Mass = (s_star * (vol * rho_pot)).transpose('time', 'k', 'face', 'j', 'i')  # kg\n",
    "\n",
    "\n",
    "# 7.2 表面体积通量（m^3/s），由淡水通量（m/s）× 面积\n",
    "freshwater = (sflux.oceFWflx - clim['sflux'].oceFWflx) * rA / rhoconst  # m^3/s\n",
    "\n",
    "# 7.3 质量平流 & GM 体积通量（m^3/s）\n",
    "chunk_dict = dict(zip(tsdaily.THETA.dims, [c[0] for c in tsdaily.THETA.chunks]))\n",
    "template = xr.Dataset({\n",
    "    'utrans': xr.zeros_like(tsdaily.THETA ).chunk(chunk_dict),\n",
    "    'utrans_right': xr.zeros_like(tsdaily.THETA ).chunk(chunk_dict),\n",
    "    'vtrans': xr.zeros_like(tsdaily.THETA ).chunk(chunk_dict),\n",
    "    'vtrans_up': xr.zeros_like(tsdaily.THETA ).chunk(chunk_dict), })\n",
    "\n",
    "# 并行计算\n",
    "ECCOgriduse = xr.Dataset({\n",
    "    'drF': ECCOgrid['drF'].astype('float64'),\n",
    "    'dyG': ECCOgrid['dyG'].astype('float64'),\n",
    "    'dxG': ECCOgrid['dxG'].astype('float64'), })\n",
    "g_adv = xr.map_blocks(\n",
    "    tab.save_G_adv_surfacevolume_ds,\n",
    "    volflux,\n",
    "    kwargs={'ECCOgrid': ECCOgriduse, 'face_connections': face_connections, 'var_prefix': 'MASS'},\n",
    "    template=template.drop_vars([\"XC\", \"YC\", \"Z\"]))     # m^3/s\n",
    "g_adv = g_adv.load()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d993a6-3632-414c-84e8-81fd1f112b1c",
   "metadata": {},
   "source": [
    "## calculation of U_prime_T_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a488a76c-8a3b-409e-8bc5-9f741539111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "## residual velocity\n",
    "ures = volflux.UVELMASS + vstar.UVELSTAR\n",
    "vres = volflux.VVELMASS + vstar.VVELSTAR\n",
    "wres = volflux.WVELMASS + vstar.WVELSTAR\n",
    "\n",
    "uprime = (ures - (clim['vol'].UVELMASS + clim['vstar'].UVELSTAR)).rename('UVELMASS_anom')\n",
    "vprime = (vres - (clim['vol'].VVELMASS + clim['vstar'].VVELSTAR)).rename('VVELMASS_anom')\n",
    "wprime = (wres - (clim['vol'].WVELMASS + clim['vstar'].WVELSTAR)).rename('WVELMASS_anom')\n",
    "\n",
    "# prepare template\n",
    "chunk_dict = dict(zip(tsdaily.THETA.dims, [c[0] for c in tsdaily.THETA.chunks]))\n",
    "template = xr.Dataset( { \"G_Hadv\": xr.zeros_like(tsdaily[\"THETA\"]).chunk(chunk_dict),\n",
    "                         \"G_Vadv\": xr.zeros_like(tsdaily[\"THETA\"]).chunk(chunk_dict), })\n",
    "# uprime Tbar\n",
    "ds_in = xr.Dataset({   \"THETA\":  clim[\"tsdaily\"].THETA,\n",
    "                       \"sTHETA\": clim[\"tsdailyS\"].THETA,\n",
    "                       \"u\": uprime, \"v\": vprime, \"w\": wprime } )\n",
    "G_upTb = xr.map_blocks( tab.cal_GMlike_prime_transport_ds, ds_in,\n",
    "             kwargs={\"face_connections\": face_connections, \"ECCOgrid\": ECCOgrid},\n",
    "             template=template.drop_vars([\"XC\", \"YC\", \"Z\"]) ) # oC * m^3/s\n",
    "G_upTb.load()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44ace0-dd60-4048-b846-4daf2a17b299",
   "metadata": {},
   "source": [
    "## calculation of Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e352226-4695-40dd-b31f-93a70fa05df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#\n",
    "vars_needed = list(clim['tadv'].data_vars)\n",
    "tbudget_prime = tbudget[vars_needed] - clim['tadv']  # oC/s（或与源数据单位一致）\n",
    "template = xr.Dataset({\n",
    "    \"dif_hConvH\": xr.zeros_like(tsdaily[\"THETA\"]).chunk(chunk_dict),\n",
    "    \"dif_vConvH\": xr.zeros_like(tsdaily[\"THETA\"]).chunk(chunk_dict), })\n",
    "G_diff = xr.map_blocks( tab.cal_T_diffusion_ds, tbudget_prime,\n",
    "            kwargs={\"face_connections\": face_connections},\n",
    "            template=template.drop_vars([\"XC\", \"YC\", \"Z\"]) )\n",
    "G_diff = G_diff.load()\n",
    "#\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4571b-c873-4548-99c0-db9d1fd47923",
   "metadata": {},
   "source": [
    "## calculation of heat forcing\n",
    "#### note that Geoheating is 0 since it doesn't have anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da940fc1-d02e-4889-9e17-dd0f7ae17301",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 7.7 表面热通量强迫（先做去气候态）\n",
    "vars_needed = ['oceQsw','TFLUX']\n",
    "hflux_prime = hflux[vars_needed] - clim['hflux'][vars_needed]     # W/m^2（或源数据单位）\n",
    "G_forcing = tab.cal_T_forcing(hflux_prime, ECCOgrid, GEOFLX=0.0)  # oC/s\n",
    "# 把 oC/s × 体积 -> oC * m^3/s（与其他通量单位一致后再乘 rho_pot 变成 oC * kg/s）\n",
    "G_heat = (G_forcing * vol).transpose('time', 'k', 'face', 'j', 'i')  # oC * m^3/s\n",
    "#\n",
    "G_heat = G_heat.load()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3830608-0c2d-4156-af75-5e1e221409a7",
   "metadata": {},
   "source": [
    "## collecting all anomaly budgets and deal with units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c96ba-62f9-4dc5-aba8-d51a77e39184",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 8) 汇总到 Dataset（统一乘上 rho_pot，得到 kg/s 或 oC*kg/s）\n",
    "ds_budgets = xr.Dataset(\n",
    "     data_vars=dict(\n",
    "            g_fw      =freshwater * rho_pot,\n",
    "            g_UpTb_h  =G_upTb.G_Hadv * rho_pot,\n",
    "            g_UpTb_v  =G_upTb.G_Vadv * rho_pot,\n",
    "            #g_UbTp_h  =G_Hadv_ubar_tprime * rho_pot,\n",
    "            #g_UbTp_v  =G_Vadv_ubar_tprime * rho_pot,  \n",
    "            #g_uptp_h  =G_Hadv_uprime_tprime * rho_pot,\n",
    "            #g_uptp_v  =G_Vadv_uprime_tprime * rho_pot,\n",
    "            g_UpTp_h  =clim['eddyforcing'].Hnabla_eddy * rho_pot,\n",
    "            g_UpTp_v  =clim['eddyforcing'].Vnabla_eddy * rho_pot,\n",
    "            g_mix_h   =G_diff.dif_hConvH * rho_pot,\n",
    "            g_mix_v   =G_diff.dif_vConvH * rho_pot,\n",
    "            g_heat    =G_heat  * rho_pot\n",
    "        ),\n",
    "        coords=dict(\n",
    "            time=tbudget.time,\n",
    "            k=ECCOgrid.k,\n",
    "            face=ECCOgrid.face,\n",
    "            j=ECCOgrid.j,\n",
    "            i=ECCOgrid.i\n",
    "        ),\n",
    "        attrs=dict(\n",
    "            note=\"All *g_* terms are fluxes on ECCO v4r4 llc90 native grid. \"\n",
    "                 \"Units: fluxes [kg/s] or [°C·kg/s].\"\n",
    "        ))\n",
    "\n",
    "g_adv     =g_adv       * rho_pot  # kg/s\n",
    "\n",
    "# Mass 单独存\n",
    "ds_budgets['Mass'] = xr.DataArray(\n",
    "        Mass,\n",
    "        dims=('time_mass','k','face','j','i'),\n",
    "        coords=dict(\n",
    "            time_mass=Mass.time.values,  # Mass 的 time (比 flux 多一天)\n",
    "            k=ECCOgrid.k,\n",
    "            face=ECCOgrid.face,\n",
    "            j=ECCOgrid.j,\n",
    "            i=ECCOgrid.i\n",
    "        ),\n",
    "        attrs=dict(units=\"kg\", note=\"State variable with extended time axis for trend calculation.\") )\n",
    "\n",
    "ds_budgets = ds_budgets.load()\n",
    "#\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2101342-ca84-4855-b891-122df19b3581",
   "metadata": {},
   "source": [
    "## calculation of Tprime snapshot and region mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b95ce-dc6f-4639-989c-ea98fa6baa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# wmb using transform\n",
    "Tsnap_prime = tscache['THETA'] - clim_tsdaily['THETA']\n",
    "maskC = ECCOgrid.maskC.copy()\n",
    "region_mask = mask & maskC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131efd6b-a82e-4837-8bf8-484dba3582b2",
   "metadata": {},
   "source": [
    "## make a temperature coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfe47f-7364-4ad1-b2a0-ada34dc53429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️⃣ 取出所有非 NaN 温度样本\n",
    "T_flat = Tsnap_prime.values.flatten()\n",
    "T_flat = T_flat[~np.isnan(T_flat)]\n",
    "# 2️⃣ 按分位数确定边界（比如分成 100 个 bin）\n",
    "n_bins = 100\n",
    "quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "t_edges = np.quantile(T_flat, quantiles)\n",
    "# 3️⃣ 根据边界计算中心值\n",
    "tcenters = 0.5 * (t_edges[:-1] + t_edges[1:])\n",
    "dtcenters = np.diff(t_edges)  # 每个 bin 的温度宽度（非均匀）\n",
    "# 4️⃣ 转成 DataArray\n",
    "tcen_outer = xr.DataArray(t_edges, dims=['tcenter'])\n",
    "#dtcenters = xr.DataArray(dtcenters, dims=['itcenter'],coords={\"itcenter\":np.arange(len(dtcenters))})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93a38a-0445-40d4-a4cb-7c7610f5bce7",
   "metadata": {},
   "source": [
    "# calculation of Tprime snapshot at the interfaces (k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f21bdb-b7e6-4e44-a9c6-79c3f2271634",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "Tsnap_prime_region_outer = grid_snap.interp(Tsnap_prime.ffill(dim=\"k\",limit=1), 'Z', boundary='extend').load()\n",
    "Tsnap_prime_region_outer_extended = tab.add_bottom_layer_from_cell( Tsnap_prime_region_outer, Tsnap_prime, ECCOgrid.k_p1, ECCOgrid.Zp1)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "print(f\"计算耗时：{elapsed:.2f} 秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e48b39-682f-4329-a7d7-6b731a552499",
   "metadata": {},
   "source": [
    "## layered budgets by using transform and do cumulative calculations similar to  [ T > Tcenter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12676e22-2051-4686-8dd3-eb79c30a4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === transform terms\n",
    "layered_results = {name: [] for name in [\n",
    "        \"g_mix_h\",\"g_mix_v\",\"g_UpTb_h\",\"g_UpTb_v\",\"g_UpTp_h\",\"g_UpTp_v\",\n",
    "        \"g_heat\",\"g_fw\",\"g_adv\",\"Mass\" ]}\n",
    "#\n",
    "for varname in [\"g_mix_h\",\"g_mix_v\",\"g_heat\",\"g_UpTb_h\",\"g_UpTb_v\",\"g_UpTp_h\",\"g_UpTp_v\"]:\n",
    "    layered_results[varname] = grid_daily.transform( ds_budgets[varname], 'Z', \n",
    "                            target=tcen_outer,\n",
    "                            method='conservative',\n",
    "                            target_data=Tsnap_prime_region_outer_extended.isel(time=slice(0,-1)))\n",
    "#\n",
    "layered_results[\"Mass\"] = grid_snap.transform( ds_budgets[\"Mass\"].rename(time_mass=\"time\"), 'Z', \n",
    "                            target=tcen_outer,\n",
    "                            method='conservative',\n",
    "                            target_data=Tsnap_prime_region_outer_extended )\n",
    "#\n",
    "# expand g_fw\n",
    "g_fw_expanded = ds_budgets[\"g_fw\"].expand_dims(k=ds_budgets[\"Mass\"].k).transpose(\"time\",\"k\",\"face\",\"j\",\"i\")\n",
    "# 给其他层赋值为0，只保留表层\n",
    "g_fw_expanded = g_fw_expanded.where(g_fw_expanded.k == 0, 0)\n",
    "\n",
    "layered_results[\"g_fw\"] = grid_daily.transform( g_fw_expanded,\n",
    "                            'Z', \n",
    "                            target=tcen_outer,\n",
    "                            method='conservative',\n",
    "                            target_data=Tsnap_prime_region_outer_extended.isel(time=slice(0,-1)) )\n",
    "##\n",
    "# -----------------------------\n",
    "layered_cum_budgets = {name: [] for name in [\n",
    "        \"g_mix_h\",\"g_mix_v\",\"g_UpTb_h\",\"g_UpTb_v\",\"g_UpTp_h\",\"g_UpTp_v\",\n",
    "        \"g_heat\",\"g_tend\",\"g_fw\",\"g_adv\" ]}\n",
    "\n",
    "for varname in [\"g_mix_h\",\"g_mix_v\",\"g_heat\",\"g_UpTb_h\",\"g_UpTb_v\",\"g_UpTp_h\",\"g_UpTp_v\"]:\n",
    "    layered_cum_budgets[varname] = vertical_pairwise_avg(layered_results[varname], layered_results[varname].tcenter, \n",
    "                                                         dtcenters,  dim=\"tcenter\", sum_dims=(\"face\", \"j\", \"i\"))\n",
    "#\n",
    "# cumulative along \">\" tcenter\n",
    "#\n",
    "dt_seconds = 86400.\n",
    "Mass_cum = layered_results[\"Mass\"].sum(['i','j','face']).isel(tcenter=slice(None, None, -1)).cumsum(dim='tcenter').isel(tcenter=slice(None, None, -1))\n",
    "layered_cum_budgets[\"g_tend\"] = Mass_cum.diff('time') / dt_seconds\n",
    "layered_cum_budgets[\"g_tend\"] = layered_cum_budgets[\"g_tend\"].assign_coords({'time': Mass_cum.time[0:-1]})\n",
    "#\n",
    "layered_cum_budgets[\"g_fw\"] = layered_results[\"g_fw\"].sum(['i','j','face']).isel(tcenter=slice(None, None, -1)).cumsum(dim='tcenter').isel(tcenter=slice(None, None, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30de31-524e-4fb3-882b-c82faae78fed",
   "metadata": {},
   "source": [
    "## see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ee3dd-0cc7-48d1-adf9-b7de3bc18df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rhs_vars = [\n",
    "    'g_mix_h', 'g_mix_v',\n",
    "    'g_UpTb_h', 'g_UpTb_v',\n",
    "    'g_UpTp_h', 'g_UpTp_v',\n",
    "    'g_heat','g_fw']\n",
    "\n",
    "t_layer = 0.15  # oC\n",
    "g_tend_layer = layered_cum_budgets['g_tend'].sel(tcenter=t_layer, method='nearest')\n",
    "rhs_layer = sum(layered_cum_budgets[var].sel(tcenter=t_layer, method='nearest') for var in rhs_vars)\n",
    "res = g_tend_layer - rhs_layer\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(g_tend_layer.time, g_tend_layer, label='g_tend')\n",
    "plt.plot(rhs_layer.time, rhs_layer, label='rhs (sum of other terms)')\n",
    "plt.plot(res.time, res, label='residual', linestyle='--')\n",
    "plt.axhline(0, color='k', linestyle=':')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('value (oC/s)')\n",
    "plt.title(f'Check g_tend vs RHS at tcenter ~ {t_layer}°C')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
